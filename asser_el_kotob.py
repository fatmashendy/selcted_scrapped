# -*- coding: utf-8 -*-
"""asser_el_kotob (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Um_qE8yp1W2aO16tzLCS_Mx1szFodLwv
"""

!pip install ar-corrector
!pip install bs4
!pip install beautifulsoup4

import requests
from bs4 import BeautifulSoup 
import pandas as pd

from ar_corrector.corrector import Corrector
corr = Corrector()

# Get the data of book card 
books_name_list = []
books_author_list = []
Books_links_list = []
# Get the detailed data of book card
author_link_list = []
department_list = []
publishing_houses_list = []
publishing_year_list = []
summary_of_books_list = []
no_of_pages_list = []
keywords = []

for page_number in range(1,400):

  result = requests.get(f"https://www.aseeralkotb.com/books?page={page_number}")
    # print result displayed response : 200 succeed 
  src = result.content
    # to extract data
  

  soup = BeautifulSoup(src)
  #soup.encode('ASCII')
  Books_links = soup.find_all('a',{'class':'hover:no-underline'})
  Books_name = soup.find_all('h1',{'class':'flex justify-center items-center text-sm font-extrabold leading-loose text-gray-600 truncate transition-colors duration-300 ease-out secondary-font-bold group-hover:text-gray-700','itemprop':'name'})

#--------------------------
  
  for i in range(len(Books_name)):    
    

    books_name_list.append(Books_name[i].text)
    books_name_list = [item.replace("\n","") for item in books_name_list ]
    books_name_list = [item.replace("-","") for item in books_name_list ]
    Books_links_list.append(Books_links[i].attrs["href"])
    #url.append(Books_links_list[i])
    #url = re.findall(re_equ, Books_links_list[i])


#print(books_name_list)
#print(Books_links_list)

# Get data of every link
for link in Books_links_list:

  result = requests.get(link)
  src = result.content
  soup = BeautifulSoup(src,"lxml")


  author_link = soup.find('a',{'class':'font-bold transition-colors duration-300 ease-out text-blueGray-700 hover:text-blueGray-900 hover:no-underline'})
  #author_link_list.append(author_link.text.strip())
  #print(author_link)
  if author_link is not None:
      
      author_link_list.append(author_link.text)
  #print(author_link_list)

  publishing_year = soup.find('dd',{'class':'col-span-3 p-3 transition-colors duration-300 ease-out bg-blueGray-50 text-blueGray-700 hover:text-blueGray-800 hover:no-underline','itemprop':'datePublished'})
  if publishing_year is not None:
      
      publishing_year_list.append(publishing_year.text)
     
  #print(publishing_year)

  publishing_house = soup.find('dd',{'class':'col-span-3 p-3 border-b border-transparent bg-blueGray-50'})
  
  if publishing_house is not None:
  
      publishing_houses_list.append(publishing_house.text)
      publishing_houses_list = [item.replace("\n","") for item in publishing_houses_list ]
  #print(publishing_house)

  department = soup.find('span',{'itemprop':'genre'})
  
  if department is not None:
  
      department_list.append(department.text)
  #print(department)

  summary_of_books = soup.find('summary',{'class':'text-sm leading-8 text-gray-900 font-semibold select-none'})
  
  if summary_of_books is not None:
  
      summary_of_books_list.append(corr.contextual_correct(summary_of_books.text))
      summary_of_books_list = [item.replace("\n","") for item in summary_of_books_list ]
      #keywords.append(custom_kw_extractor.extract_keywords(summary_of_books.text))
  #print(summary_of_books)

  no_of_pages = soup.find('dd',{'class':'col-span-3 p-3 transition-colors duration-300 ease-out bg-blueGray-50 text-blueGray-700 hover:text-blueGray-800 hover:no-underline','itemprop':'numberOfPages'})

  if no_of_pages is not None:
    no_of_pages_list.append(no_of_pages.text)
  




'''
print(author_link_list)
print(publishing_year_list)
print(publishing_houses_list)
print(department_list)
print(summary_of_books_list)
'''

df = pd.DataFrame(list(zip(books_name_list,author_link_list,publishing_year_list,publishing_houses_list,department_list,summary_of_books_list,no_of_pages_list)),columns =['الكتب','الكاتب',  'سنة النشر','دار النشر' , 'قسم الكتاب' ,'تلخيص الكتاب' , 'عدد الصفحات'])
df.to_csv('sample_data/test.csv',index=False,header=True,encoding="UTF-8")

data = pd.DataFrame(list(zip(keywords)),columns=['list_keywords'])

data

